# LLM Provider Configuration
# Options: "deepseek" (cloud API) or "lmstudio" (local)
LLM_PROVIDER=deepseek

# DeepSeek API Key (required if LLM_PROVIDER=deepseek)
# Get your API key from: https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=your_api_key_here

# LM Studio Configuration (required if LLM_PROVIDER=lmstudio)
# Make sure LM Studio is running locally with a model loaded
# Default: http://127.0.0.1:1234/v1
LMSTUDIO_BASE_URL=http://127.0.0.1:1234/v1

# LM Studio Model ID (optional, defaults to meta-llama-3.1-8b-instruct)
# Change this if you're using a different model in LM Studio
LMSTUDIO_MODEL_ID=meta-llama-3.1-8b-instruct
